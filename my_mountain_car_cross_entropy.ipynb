{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car With Cross Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward used in this mountain car problem is:<br />\n",
    "$total\\_reward = total\\_reward + reward + (feature\\_scaling\\_value\\_from\\_range\\_[-1.2,0.6]\\_to\\_range\\_[0,1](position))^2$<br />\n",
    "However this reward is not able to learn a good policy.  In this problem, each state's reward is -1.0, except the state that the car reaches the goal position.  This makes exploration become important.  Within the limited (200) steps, this method is not able to explore enough states to reach goal state (or states close to goal state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: car reaches the flag on top of the mountain on the right side (position value: 0.5) within 200 time steps\n",
    "# action: 0 (acceleration towards left), 1 (stay), 2 (acceleration towards right)\n",
    "action_size = 3\n",
    "# observation (state): [position (initial value: uniformly sample from range [-0.6, -0.4], minimum value: -1.2, maximum value: 0.6), velocity (initial value: 0.0, minimum value: -0.07, maximum value: 0.07)]\n",
    "state_size = 2\n",
    "# other environment hyper parameters\n",
    "hidden_layer_size = 128\n",
    "batch_size = 25\n",
    "learning_rate = 0.01\n",
    "max_episodes = 100\n",
    "max_steps = 200\n",
    "percentile = 70\n",
    "max_num_of_trials = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial code below is derived from repository: https://github.com/schneider128k/reinforcement/\n",
    "# neural network\n",
    "class Net:\n",
    "    def __init__(self, \n",
    "                 state_size = state_size, \n",
    "                 action_size = action_size, \n",
    "                 hidden_layer_size = hidden_layer_size,\n",
    "                 learning_rate = learning_rate, \n",
    "                 name = 'net'):\n",
    "        with tf.variable_scope(name):\n",
    "            ### Prediction part\n",
    "            \n",
    "            # Input layer, state s is input\n",
    "            self.states = tf.placeholder(\n",
    "                tf.float32, \n",
    "                [None, state_size])\n",
    "        \n",
    "            # Hidden layer, ReLU activation\n",
    "            self.hidden_layer = tf.contrib.layers.fully_connected(\n",
    "                self.states, \n",
    "                hidden_layer_size)\n",
    "            \n",
    "            # Hidden layer, linear activation, logits\n",
    "            self.logits = tf.contrib.layers.fully_connected(\n",
    "                self.hidden_layer, \n",
    "                action_size,\n",
    "                activation_fn = None)\n",
    "            \n",
    "            # Output layer, softmax activation yields probability distribution for actions\n",
    "            self.probabilities = tf.nn.softmax(self.logits)\n",
    "    \n",
    "            ### Training part \n",
    "    \n",
    "            # Action a\n",
    "            self.actions = tf.placeholder(\n",
    "                tf.int32, \n",
    "                [None])\n",
    "            \n",
    "            # One-hot encoded action a \n",
    "            #\n",
    "            # encoded_action_vector = [1, 0, 0] if action a = 0\n",
    "            # encoded_action_vector = [0, 1, 0] if action a = 1\n",
    "            # encoded_action_vector = [0, 0, 1] if action a = 2\n",
    "            self.one_hot_actions = tf.one_hot(\n",
    "                self.actions, \n",
    "                action_size)\n",
    "\n",
    "            # cross entropy\n",
    "            self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits = self.logits, \n",
    "                labels = self.one_hot_actions)\n",
    "            \n",
    "            # cost\n",
    "            self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "            \n",
    "            # Optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "            \n",
    "    # get action chosen according to current probabilistic policy\n",
    "    def get_action(self, state):\n",
    "        feed_dict = { self.states : np.array([state]) } \n",
    "        probabilities = sess.run(self.probabilities, feed_dict = feed_dict)\n",
    "        \n",
    "        return np.random.choice(action_size, p=probabilities[0])\n",
    "    \n",
    "    # train based on batch\n",
    "    def train(self, batch):\n",
    "        states, actions = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.states : states,\n",
    "            self.actions : actions\n",
    "        }\n",
    "        \n",
    "        sess.run(self.optimizer, feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward function\n",
    "def get_reward(position, velocity):\n",
    "    pos_min = -1.2\n",
    "    pos_max = 0.6\n",
    "    pos_cur = (position - pos_min) / (pos_max - pos_min)\n",
    "    return math.pow(pos_cur, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Test: 1, Reward:-169.58, Timestep:200\n",
      "Test: 2, Reward:-170.84, Timestep:200\n",
      "Test: 3, Reward:-171.38, Timestep:200\n",
      "Test: 4, Reward:-169.29, Timestep:200\n",
      "Test: 5, Reward:-166.83, Timestep:200\n",
      "Test: 6, Reward:-162.09, Timestep:200\n",
      "Test: 7, Reward:-166.74, Timestep:200\n",
      "Test: 8, Reward:-166.43, Timestep:200\n",
      "Test: 9, Reward:-162.59, Timestep:200\n",
      "Test:10, Reward:-161.99, Timestep:200\n",
      "Test:11, Reward:-164.78, Timestep:200\n",
      "Test:12, Reward:-164.77, Timestep:200\n",
      "Test:13, Reward:-163.69, Timestep:200\n",
      "Test:14, Reward:-163.54, Timestep:200\n",
      "Test:15, Reward:-164.20, Timestep:200\n",
      "Test:16, Reward:-161.77, Timestep:200\n",
      "Test:17, Reward:-160.99, Timestep:200\n",
      "Test:18, Reward:-163.44, Timestep:200\n",
      "Test:19, Reward:-164.83, Timestep:200\n",
      "Test:20, Reward:-162.48, Timestep:200\n"
     ]
    }
   ],
   "source": [
    "# get mountain car environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# training\n",
    "tf.reset_default_graph()\n",
    "net = Net(name = 'net',\n",
    "          hidden_layer_size = hidden_layer_size,\n",
    "          learning_rate = learning_rate)\n",
    "# run\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_index = int(max_episodes * percentile / 100)\n",
    "    \n",
    "    num_of_trials = 0\n",
    "    while num_of_trials < max_num_of_trials:\n",
    "        num_of_trials += 1\n",
    "\n",
    "        total_reward_list = []\n",
    "        trajectory_list = []\n",
    "\n",
    "        for e in np.arange(max_episodes):\n",
    "            total_reward = 0.0\n",
    "            trajectory = []\n",
    "            state = env.reset()\n",
    "            for s in np.arange(max_steps):\n",
    "                action = net.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                # get car position and velocity\n",
    "                position, velocity = next_state\n",
    "                # update car reward\n",
    "                total_reward += reward + get_reward(position, velocity)\n",
    "                trajectory.append((state, action))\n",
    "                state = next_state\n",
    "                if done: break\n",
    "\n",
    "            index = bisect.bisect(total_reward_list, total_reward)\n",
    "            total_reward_list.insert(index, total_reward)\n",
    "            trajectory_list.insert(index, trajectory)\n",
    "        \n",
    "        # keep the elite episodes, that is, throw out the bad ones \n",
    "        # train on state action pairs extracted from the elite episodes\n",
    "        state_action_pairs = []\n",
    "        for trajectory in trajectory_list[start_index:]:\n",
    "            for state_action_pair in trajectory:\n",
    "                state_action_pairs.append(state_action_pair)\n",
    "        # shuffle to avoid correlations between adjacent states\n",
    "        random.shuffle(state_action_pairs) \n",
    "        n = len(state_action_pairs)\n",
    "        batches = [state_action_pairs[k:k + batch_size] for k in np.arange(0, n, batch_size)]\n",
    "\n",
    "        for batch in batches:\n",
    "            net.train(batch)\n",
    "\n",
    "        # test episode runs\n",
    "        total_reward = 0\n",
    "        time_step = 0\n",
    "        observation = env.reset()\n",
    "        for time_step in range(max_steps):\n",
    "            env.render()\n",
    "            action = net.get_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            position, velocity = observation\n",
    "            #print(f\"P:{position}, V:{velocity}, D:{done}, T:{time_step}\")\n",
    "            total_reward += reward + get_reward(position, velocity)\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Test:{num_of_trials:2d}, Reward:{total_reward:5.2f}, Timestep:{time_step + 1:3d}\")\n",
    "        \n",
    "# close mountain car environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
