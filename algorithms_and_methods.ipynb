{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Methods Overview ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Contents covered below are from class slides and notes)<br />\n",
    "#### Uninformed Search: \n",
    "An uninformed search algorithm generates a tree (tree search may have repeated structures, causing exponential work) or a graph (graph search has each state occurring only once) without using any domain specific knowledge to perform search.  Examples are: depth first search (expanding a deepest node first), breadth first search (expanding a shallowest node first), and uniform cost search (expanding a cheapest node (cumulative backward cost) first).\n",
    "<br />\n",
    "#### Informed Search:\n",
    "An informed search algorithm uses domain specific knowledge (e.g. a heuristic function estimates how close a state is to a goal) to perform search (by constructing either a tree or a graph). Examples are: greedy search (expanding a node closest to the goal (smallest forward cost)), and A* search (expanding a node having the smallest cost of the summation of backward cost and forward cost).  An optimal solution is achievable if the heuristic function is admissible and consistent.\n",
    "<br />\n",
    "#### Adversarial Search:\n",
    "An adversarial search algorithm calculates a strategy (or policy) for a recommended action from each state in a game for one of the players.  Examples are: minimax search (constructing a state-space search tree and computing each node's best achievable utility (max node computes maximum score; min node computes minimum score) against a rational (worst-case scenario) adversary recursively) with alpha-beta pruning (decreasing the number of nodes needed to be evaluated), and expectimax search (constructing a state-space search tree and computing each node's average utility (max node computes maximum score; chance node computes weighted average (expected) score) against a random (average-case scenario) adversary recursively).\n",
    "<br />\n",
    "#### Markov Decision Process:\n",
    "A Markov decision process (MDP) is a non-determinstic algorithm to solve search problems.  It constructs a search tree with states (i.e. s) and q-states (i.e. (s,a)), including states, available actions of each state, a transition function (probability of performing an action from one state to another state), a reward function (additive or discount reward of performing an action from one state to another state), and runs from a given start state.  Below are recursive definitions of how to compute MDP values via the Bellman equations:\n",
    "<br />\n",
    "Value Iteration: \n",
    "<br />\n",
    "$V^*(s)=\\underset{a}{max}\\underset{s'}{\\sum}T(s,a,s')[R(s,a,s') + \\gamma V^*(s')]$\n",
    "<br />\n",
    "Policy Evaluation: \n",
    "<br />\n",
    "$V^{\\pi}(s)=\\underset{s'}{\\sum}T(s,\\pi(s),s')[R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$\n",
    "<br />\n",
    "Policy Extraction (Improvement): \n",
    "<br />\n",
    "$\\pi(s)=\\underset{a}{argmax}\\underset{s'}{\\sum}T(s,a,s')[R(s,a,s') + \\gamma V(s')]$\n",
    "<br />\n",
    "Policy Iteration:\n",
    "<br />\n",
    "Step 1: run policy evaluation on the first randomly generated policy unitl it converges. <br />\n",
    "Step 2: run policy extraction (using one-step look-ahead to get best actions) on the result policy to get an updated policy. <br />\n",
    "Repeat above steps until policy converges. <br />\n",
    "This method guarantees an optimal policy, and converges faster than using value iteration method.\n",
    "<br />\n",
    "#### Reinforcement Learning:\n",
    "Reinforcement learning (RL) is an online method (compared to MDP as an offline method) of agent learning by taking actions and receiving feedbacks in the form of rewards in order to maximize expected rewards.  An example of active (full) RL is Q-Learning.  Without knowing the transitions and rewards, the optimal policy is learned by computing the average q-value on the run via a sample-based Q-Value Iteration method:\n",
    "<br />\n",
    "$Q(s,a)=\\underset{s'}{\\sum}T(s,a,s')[R(s,a,s') + \\gamma \\underset{a'}{max}Q(s',a')]$\n",
    "<br />\n",
    "The q-value is composed of the old estimated q-value is $Q(s,a)$ and the new sample estimated value: $R(s,a,s') + \\gamma \\underset{a'}{max}Q(s',a')$, weighted by the learning rate $\\alpha$: \n",
    "<br />\n",
    "$Q(s,a)=(1-\\alpha)Q(s,a)+\\alpha [R(s,a,s') + \\gamma \\underset{a'}{max}Q(s',a')]$.\n",
    "<br />\n",
    "To balance the exploration and exploitation during the learning process, a greedy probability $\\epsilon$ is introduced: agent acts randomly with a small probability $\\epsilon$; agent acts on current policy with a large probability $1-\\epsilon$.\n",
    "<br />\n",
    "Since there could be infinite number of continuous states and actions for a given problem, a tabular q-learning method may not be able to hold such amount of data.  An approximate q-learning with linear feature extraction may be a more practical method in this case:\n",
    "<br />\n",
    "$Q(s,a)=\\sum^n_{i=1}w_if_i(s,a), \\: where \\: w_i=w_i+\\alpha[r+\\gamma \\underset{a'}{max}Q(s',a')]f_i(s,a)$\n",
    "<br />\n",
    "Besides active RL methods, there are many passive RL methods.  For example, a model-based passive RL method constructs an underlying MDP continually with observed samples.  There are some model-free passive RL methods: direct evaluation, temporal difference learning, etc., and some policy-based RL methods: policy gradient, cross entropy, etc.  Other useful approaches are: random search, hill climbing, and non-linear neural network q-learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
