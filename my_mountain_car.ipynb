{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car With an Explicit Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code blue print is from repository: \n",
    "https://github.com/schneider128k/reinforcement/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get mountain car environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "# goal: car reaches the flag on top of the mountain on the right side\n",
    "# action: 0 (acceleration towards left), 1 (stay), 2 (acceleration towards right)\n",
    "# observation (state): [position (initial value: -0.5), velocity (initial value: 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode():\n",
    "    total_reward = 0\n",
    "    time_step = 0\n",
    "    observation = env.reset()\n",
    "    for time_step in range(1000):\n",
    "        env.render()\n",
    "        # get position and velocity from observation (state)\n",
    "        position, velocity = observation\n",
    "        #print(\"Position: \", position, \", Velocity: \", velocity)\n",
    "        # apply an explicit policy\n",
    "        if velocity < 0:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 2\n",
    "        #action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #print(\"Observation: \", observation, \", Reward: \", reward, \", Done: \", done, \"Info: \", info)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            #print(\"Episode \", episode, \" finished after \", time_step + 1, \" timesteps\")\n",
    "            break\n",
    "    return total_reward, time_step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0, Reward:-122.0, Timestep:122\n",
      "Episode:1, Reward:-118.0, Timestep:118\n",
      "Episode:2, Reward:-115.0, Timestep:115\n",
      "Episode:3, Reward:-124.0, Timestep:124\n",
      "Episode:4, Reward:-122.0, Timestep:122\n",
      "Episode:5, Reward:-123.0, Timestep:123\n",
      "Episode:6, Reward:-114.0, Timestep:114\n",
      "Episode:7, Reward:-123.0, Timestep:123\n",
      "Episode:8, Reward:-122.0, Timestep:122\n",
      "Episode:9, Reward:-117.0, Timestep:117\n"
     ]
    }
   ],
   "source": [
    "# test episode runs\n",
    "for episode in range(10):\n",
    "    total_reward, time_step = run_episode()\n",
    "    #print(\"Episode:\", episode, \", Reward:\", total_reward, \", Timestep:\", time_step)\n",
    "    print(f\"Episode:{episode}, Reward:{total_reward}, Timestep:{time_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close mountain car environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
